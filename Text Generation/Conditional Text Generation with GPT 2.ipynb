{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:29:56.828436Z",
     "iopub.status.busy": "2020-08-27T01:29:56.827544Z",
     "iopub.status.idle": "2020-08-27T01:29:56.830623Z",
     "shell.execute_reply": "2020-08-27T01:29:56.830044Z"
    },
    "papermill": {
     "duration": 0.015843,
     "end_time": "2020-08-27T01:29:56.830744",
     "exception": false,
     "start_time": "2020-08-27T01:29:56.814901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007726,
     "end_time": "2020-08-27T01:29:56.845215",
     "exception": false,
     "start_time": "2020-08-27T01:29:56.837489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experimenting with HuggingFace - Text Generation\n",
    "\n",
    "*Author: Tucker Arrants*\n",
    "\n",
    "**I have recently decided to explore the ins and outs of the ðŸ˜Š Transformers library and this is the next chapter in that journey. In this notebook, I will explore text generation using a GPT-2 model, which was trained to predict next words on 40GB of Internet text data. The fully trained model is actually not available as the creators were concerned about '[malicious applications of the technology](https://openai.com/blog/better-language-models/)', but there is a much smaller version that is available for enthusiants to play with, which we will use here**\n",
    "\n",
    "**In this notebook, we will explore different decoding methods like Beam search, Top-K sampling, and Top-P sampling, demonstrating their performance along the way. This project is a work in progress and I will continually update it as I learn more about text generation. Feel free to comment with any questions/suggestions:**\n",
    "\n",
    "**Update: they just released the GPT-3 model (more [here](https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/)) and it has 175 billion parameters and it is shockingly intelligent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:29:56.864502Z",
     "iopub.status.busy": "2020-08-27T01:29:56.862781Z",
     "iopub.status.idle": "2020-08-27T01:29:56.865117Z",
     "shell.execute_reply": "2020-08-27T01:29:56.865692Z"
    },
    "papermill": {
     "duration": 0.014061,
     "end_time": "2020-08-27T01:29:56.865831",
     "exception": false,
     "start_time": "2020-08-27T01:29:56.851770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for reproducability\n",
    "SEED = 34\n",
    "\n",
    "#maximum number of words in output text\n",
    "MAX_LEN = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.006254,
     "end_time": "2020-08-27T01:29:56.878719",
     "exception": false,
     "start_time": "2020-08-27T01:29:56.872465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# I. Intro\n",
    "\n",
    "**A language model is a machine learning model that can look at part of a sentence and predict the next word/sequence of words. Much like the autofill features on your iPhone/Android, GPT-2 is capable of next word prediction on a much larger and more sophisticated scale. For reference, the smallest available GPT-2 has 117 million parameters, whereas the largest one (invisible to the public) has over 1.5 billion parameters. The largest one available for public use is half the size of their main GPT-2 model**\n",
    "\n",
    "**ðŸ˜Š Transformers makes it very easy to import this model with both PyTorch and TensorFlow - in this notebook we will be using TensorFlow but it is just as easy in PyTorch. Both the model and its Tokenizer can be imported from the `transformers` library that anyone can get by typing `!pip install transformers`. Let's see just how simple it is to generate text with a neural network. We begin with our input sequence:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:29:56.895800Z",
     "iopub.status.busy": "2020-08-27T01:29:56.895097Z",
     "iopub.status.idle": "2020-08-27T01:29:56.897907Z",
     "shell.execute_reply": "2020-08-27T01:29:56.897180Z"
    },
    "papermill": {
     "duration": 0.012977,
     "end_time": "2020-08-27T01:29:56.898015",
     "exception": false,
     "start_time": "2020-08-27T01:29:56.885038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_sequence = \"I don't know about you, but there's only one thing I want to do after a long day of work\""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.00685,
     "end_time": "2020-08-27T01:29:56.911851",
     "exception": false,
     "start_time": "2020-08-27T01:29:56.905001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**We will choose the largest available GPT-2 model but it is easy to install the other sizes if you want to mess around with them:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:29:56.928251Z",
     "iopub.status.busy": "2020-08-27T01:29:56.927649Z",
     "iopub.status.idle": "2020-08-27T01:31:49.755248Z",
     "shell.execute_reply": "2020-08-27T01:31:49.754578Z"
    },
    "papermill": {
     "duration": 112.836886,
     "end_time": "2020-08-27T01:31:49.755425",
     "exception": false,
     "start_time": "2020-08-27T01:29:56.918539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aba15bea79d49d28fd25857bca39148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131887c268164cd2a6283d07e5f3d287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af0c790c94f46febd06c726829a698b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=764.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36b76cd1ea24effb43732cc57198f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3096618024.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: \"tfgp_t2lm_head_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "transformer (TFGPT2MainLayer multiple                  774030080 \n",
      "=================================================================\n",
      "Total params: 774,030,080\n",
      "Trainable params: 774,030,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#get transformers\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "#get large GPT2 tokenizer and GPT2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "#GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-medium\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#view model parameters\n",
    "GPT2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.006843,
     "end_time": "2020-08-27T01:31:49.769683",
     "exception": false,
     "start_time": "2020-08-27T01:31:49.762840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Wow, we just imported a deep learning model with more than 774 million parameters in just a couple lines of code with HuggingFace. Now let's see what we can do with it:**"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.00655,
     "end_time": "2020-08-27T01:31:49.782971",
     "exception": false,
     "start_time": "2020-08-27T01:31:49.776421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# II. Different Decoding Methods\n",
    "\n",
    "## First Pass (Greedy Search)\n",
    "\n",
    "**With Greedy search, the word with the highest probability is predicted as the next word i.e. the next word is updated via:**\n",
    "\n",
    "$$w_t = argmax_{w}P(w | w_{1:t-1})$$\n",
    "\n",
    "**at each timestep $t$. Let's see how this naive approach performs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:31:49.801873Z",
     "iopub.status.busy": "2020-08-27T01:31:49.801289Z",
     "iopub.status.idle": "2020-08-27T01:31:49.803990Z",
     "shell.execute_reply": "2020-08-27T01:31:49.803540Z"
    },
    "papermill": {
     "duration": 0.014282,
     "end_time": "2020-08-27T01:31:49.804086",
     "exception": false,
     "start_time": "2020-08-27T01:31:49.789804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get deep learning basics\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:31:49.821608Z",
     "iopub.status.busy": "2020-08-27T01:31:49.820776Z",
     "iopub.status.idle": "2020-08-27T01:32:22.577795Z",
     "shell.execute_reply": "2020-08-27T01:32:22.578453Z"
    },
    "papermill": {
     "duration": 32.767642,
     "end_time": "2020-08-27T01:32:22.578643",
     "exception": false,
     "start_time": "2020-08-27T01:31:49.811001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I don't know about you, but there's only one thing I want to do after a long day of work: go to the gym.\n",
      "\n",
      "I'm not talking about the gym that's right next to my house. I'm talking about the gym that's right next to my office.\n",
      "\n",
      "I'm not talking about the gym that\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = GPT2.generate(input_ids, max_length = MAX_LEN)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.006829,
     "end_time": "2020-08-27T01:32:22.594857",
     "exception": false,
     "start_time": "2020-08-27T01:32:22.588028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**And there we go: generating text is that easy. Our results are not great - as we can see, our model starts repeating itself rather quickly. The main issue with Greedy Search is that words with high probabilities can be masked by words in front of them with low probabilities, so the model is unable to explore more diverse combinations of words. We can prevent this by implementing Beam Search:**"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.006632,
     "end_time": "2020-08-27T01:32:22.608643",
     "exception": false,
     "start_time": "2020-08-27T01:32:22.602011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Beam Search with N-Gram Penalities\n",
    "\n",
    "**Beam search is essentially Greedy Search but the model tracks and keeps `num_beams` of hypotheses at each time step, so the model is able to compare alternative paths as it generates text. We can also include a n-gram penalty by setting `no_repeat_ngram_size = 2` which ensures that no 2-grams appear twice. We will also set `num_return_sequences = 5` so we can see what the other 5 beams looked like**\n",
    "\n",
    "**To use Beam Search, we need only modify some parameters in the `generate` function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:32:22.625196Z",
     "iopub.status.busy": "2020-08-27T01:32:22.624671Z",
     "iopub.status.idle": "2020-08-27T01:33:21.575466Z",
     "shell.execute_reply": "2020-08-27T01:33:21.576152Z"
    },
    "papermill": {
     "duration": 58.960915,
     "end_time": "2020-08-27T01:33:21.576370",
     "exception": false,
     "start_time": "2020-08-27T01:32:22.615455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
      "\n",
      "\"I know, I know,\" you say. \"But you're not going to like this one. It's not a good movie. I mean, it's\n",
      "1: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
      "\n",
      "\"I know, I know,\" you say. \"But you're not going to like this one. It's about a guy who has a crush on a girl\n",
      "2: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
      "\n",
      "\"I know, I know,\" you say. \"But you're not going to like this one. It's about a guy who has a crush on a woman\n",
      "3: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
      "\n",
      "\"I know, I know,\" you say. \"But you're not going to like this one. It's about a guy who has a crush on a beautiful\n",
      "4: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
      "\n",
      "\"I know, I know,\" you say. \"But you're not going to like this one. It's not a good movie. I'm not sure if\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = GPT2.generate(\n",
    "    input_ids, \n",
    "    max_length = MAX_LEN, \n",
    "    num_beams = 5, \n",
    "    no_repeat_ngram_size = 2, \n",
    "    num_return_sequences = 5, \n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "print('')\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "# now we have 3 output sequences\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "      print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.006918,
     "end_time": "2020-08-27T01:33:21.591736",
     "exception": false,
     "start_time": "2020-08-27T01:33:21.584818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Now that's much better! The 5 different beam hypotheses are pretty much all the same, but if we increaed `num_beams`, then we would see some more variation in the separate beams. But of course, Beam Search is not perfect either. It works well when the legnth of the generated text is more or less constant, like problems in translation or summarization, but not so much for open-ended problems like dialog or story generation (because it is much harder to find a balance between `num_beams` and `no_repeat_ngram_size`)**\n",
    "\n",
    "**Furthermore, [research](https://arxiv.org/abs/1904.09751) shows that human languages do not follow this 'high probability word next' distribution. This makes sense - if my words were exactly what you expected them to be, I would be quite a boring person and most people don't want to be boring! The below graph plots the difference of Beam Search and actual human speech: ![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)**\n",
    "\n",
    "Taken from original paper [here](https://arxiv.org/abs/1904.09751)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.00667,
     "end_time": "2020-08-27T01:33:21.605587",
     "exception": false,
     "start_time": "2020-08-27T01:33:21.598917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Basic Sampling\n",
    "\n",
    "**Now we will explore indeterministic decodings - sampling. Instead of following a strict path to find the end text with the highest probability, we instead randomly pick the next word by its conditional probability distribution:**\n",
    "\n",
    "$$w_t \\sim P(w|w_{1:t-1})$$\n",
    "\n",
    "**However, when we include this randomness, the generated text tends to be incoherent (see more [here](https://arxiv.org/pdf/1904.09751.pdf)) so we can include the `temperature` parameter which increases the chances of high probability words and decreases the chances of low probability words in the sampling:**\n",
    "\n",
    "**We just need to set `do_sample = True` to implement sampling and for demonstration purposes (you'll shortly see why) we set `top_k = 0`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:33:21.626013Z",
     "iopub.status.busy": "2020-08-27T01:33:21.625426Z",
     "iopub.status.idle": "2020-08-27T01:33:54.255874Z",
     "shell.execute_reply": "2020-08-27T01:33:54.256598Z"
    },
    "papermill": {
     "duration": 32.644265,
     "end_time": "2020-08-27T01:33:54.256781",
     "exception": false,
     "start_time": "2020-08-27T01:33:21.612516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I don't know about you, but there's only one thing I want to do after a long day of work: put on a pair of lightweight jeans. (I don't know if I even have enough jeans on to wear them for another 30 days, though.) Lucky for me, I had a pair of these in my closet along with my\n"
     ]
    }
   ],
   "source": [
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_k = 0, \n",
    "                             temperature = 0.8\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007118,
     "end_time": "2020-08-27T01:33:54.272561",
     "exception": false,
     "start_time": "2020-08-27T01:33:54.265443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Top-K Sampling\n",
    "\n",
    "**In Top-K sampling, the top k most likely next words are selected and the entire probability mass is shifted to these k words. So instead of increasing the chances of high probability words occuring and decreasing the chances of low probabillity words, we just remove low probability words all together**\n",
    "\n",
    "**We just need to set `top_k` to however many of the top words we want to consider for our conditional probability distribution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:33:54.299159Z",
     "iopub.status.busy": "2020-08-27T01:33:54.292468Z",
     "iopub.status.idle": "2020-08-27T01:34:28.035752Z",
     "shell.execute_reply": "2020-08-27T01:34:28.036509Z"
    },
    "papermill": {
     "duration": 33.75677,
     "end_time": "2020-08-27T01:34:28.036704",
     "exception": false,
     "start_time": "2020-08-27T01:33:54.279934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I don't know about you, but there's only one thing I want to do after a long day of work: I want to relax with a good book on my lap. One thing that I know for sure is that the most important factor in writing a good book is the author's ability to write a good book.\n",
      "\n",
      "So, what ...\n"
     ]
    }
   ],
   "source": [
    "#sample from only top_k most likely words\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_k = 50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007464,
     "end_time": "2020-08-27T01:34:28.053113",
     "exception": false,
     "start_time": "2020-08-27T01:34:28.045649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Top-K Sampling seems to generate more coherent text than our random sampling before. But we can do even better:**"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007438,
     "end_time": "2020-08-27T01:34:28.068384",
     "exception": false,
     "start_time": "2020-08-27T01:34:28.060946",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Top-P Sampling\n",
    "\n",
    "**Top-P sampling (also known as nucleus sampling) is similar to Top-K, but instead of choosing the top k most likely wordsm we choose the smallest set of words whose total probability is larger than p, and then the entire probability mass is shifted to the words in this set**\n",
    "\n",
    "**The main difference here is that with Top-K sampling, the size of the set of words is static (obviously) whereas in Top-P sampling, the size of the set can change. To use this sampling method, we just set `top_k = 0` and choose a value `top_p`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:34:28.089767Z",
     "iopub.status.busy": "2020-08-27T01:34:28.089137Z",
     "iopub.status.idle": "2020-08-27T01:35:02.050650Z",
     "shell.execute_reply": "2020-08-27T01:35:02.051321Z"
    },
    "papermill": {
     "duration": 33.97576,
     "end_time": "2020-08-27T01:35:02.051566",
     "exception": false,
     "start_time": "2020-08-27T01:34:28.075806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I don't know about you, but there's only one thing I want to do after a long day of work: go home.\"\n",
      "\n",
      "Mitsuha wasn't even sure what he meant. Was he saying she should do something else? After all, he was just one person. They all worked together, so why should she be so concerned ...\n"
     ]
    }
   ],
   "source": [
    "#sample only from 80% most likely words\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_p = 0.8, \n",
    "                             top_k = 0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007207,
     "end_time": "2020-08-27T01:35:02.066391",
     "exception": false,
     "start_time": "2020-08-27T01:35:02.059184",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Top-K and Top-P Sampling\n",
    "\n",
    "**As you could have probably guessed, we can use both Top-K and Top-P sampling here. This reduces the chances of us getting weird words (low probability words) while allowing for a dynamic selection size. We need only top a value for both `top_k` and `top_p`. We can even include the inital temperature parameter if we want to, Let's now see how our model performs now after adding everything together. We will check the top 5 return to see how diverse our answers are:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:35:02.087451Z",
     "iopub.status.busy": "2020-08-27T01:35:02.086858Z",
     "iopub.status.idle": "2020-08-27T01:37:22.372979Z",
     "shell.execute_reply": "2020-08-27T01:37:22.373740Z"
    },
    "papermill": {
     "duration": 140.299905,
     "end_time": "2020-08-27T01:37:22.373963",
     "exception": false,
     "start_time": "2020-08-27T01:35:02.074058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I don't know about you, but there's only one thing I want to do after a long day of work: make sure to eat some good old-fashioned bacon. There's something about bacon, you guys. It tastes good, it makes you feel good, it makes you feel good. It makes you feel full and satisfied, like you just spent an entire day of hard work.\n",
      "\n",
      "If you're not a bacon fanatic, it's worth a shot. Bacon is a delicious meat, but it's especially delicious when you make it yourself. This is a recipe for making your own bacon. And the best part? It's so easy that you can make it right in your own...\n",
      "\n",
      "1: I don't know about you, but there's only one thing I want to do after a long day of work: get some quality sleep!\n",
      "\n",
      "Sleep is essential for our wellbeing and we don't have it enough. The average adult gets 8.3 hours of sleep per night. Yet we don't all get enough of it. The World Health Organisation has called for more people to get at least 7 hours of sleep a night. And that's just in the UK.\n",
      "\n",
      "In the USA, the average person gets just 6.2 hours of sleep. And the reason for that is that many of us are not getting enough physical activity.\n",
      "\n",
      "A recent study found that more than...\n",
      "\n",
      "2: I don't know about you, but there's only one thing I want to do after a long day of work: watch anime!\"\n",
      "\n",
      "\"I bet you can, so get out there and watch some anime!\"\n",
      "\n",
      "\"I love anime!\"\n",
      "\n",
      "\"I love anime! It's one of the best things in the world!\"\n",
      "\n",
      "\"I've watched anime since I was little.\"\n",
      "\n",
      "\"I'm so jealous!\"\n",
      "\n",
      "\"I've never heard of you, but I bet you're cool!\"\n",
      "\n",
      "\"I want to be an anime character!\"\n",
      "\n",
      "\"I'd like to play anime!\"\n",
      "\n",
      "\"I want to be an anime character!\"\n",
      "\n",
      "...\n",
      "\n",
      "3: I don't know about you, but there's only one thing I want to do after a long day of work and that's get out and run!\"\n",
      "\n",
      "In addition to the regular workout schedule, her training includes long-distance walks, hiking and cross-training.\n",
      "\n",
      "Her husband, Mike, who has a background in running, is also a runner. The pair often enjoy a run to the store or a picnic together.\n",
      "\n",
      "\"It's a family thing. It's part of our daily lives,\" said Mike, a marketing executive in Oklahoma City. \"I like doing it together.\"\n",
      "\n",
      "Amber and Mike were married in a ceremony in their hometown of St. George...\n",
      "\n",
      "4: I don't know about you, but there's only one thing I want to do after a long day of work: I want to be on my knees thanking God for the wonderful people who have helped me through this difficult time.\n",
      "\n",
      "My best friend, I have been told, has been \"caught in the crossfire of religious persecution\" since I left the church. Now she will likely be put through the same agony. I would hate to think that she will be forced to make the same choice, either because she has a religious conscience or because she is so weak she is willing to give in.\n",
      "\n",
      "As she says: \"My faith has been a part of me since I...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#combine both sampling techniques\n",
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = 2*MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .7,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85, \n",
    "                              num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.00739,
     "end_time": "2020-08-27T01:37:22.393580",
     "exception": false,
     "start_time": "2020-08-27T01:37:22.386190",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# III. Benchmark Prompts\n",
    "\n",
    "**Here, we will see how well the GPT-2 model does when given some more interesting inputs. The following prompts are taken from [OpenAI's GPT2](https://openai.com/blog/better-language-models/) website where they feed them to their full sized GPT2 model (and the results are astounding, I highly recommend you check out their [page](https://openai.com/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:37:22.412828Z",
     "iopub.status.busy": "2020-08-27T01:37:22.412257Z",
     "iopub.status.idle": "2020-08-27T01:37:22.415091Z",
     "shell.execute_reply": "2020-08-27T01:37:22.414394Z"
    },
    "papermill": {
     "duration": 0.014255,
     "end_time": "2020-08-27T01:37:22.415208",
     "exception": false,
     "start_time": "2020-08-27T01:37:22.400953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007205,
     "end_time": "2020-08-27T01:37:22.430110",
     "exception": false,
     "start_time": "2020-08-27T01:37:22.422905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:37:22.451019Z",
     "iopub.status.busy": "2020-08-27T01:37:22.449076Z",
     "iopub.status.idle": "2020-08-27T01:37:22.454274Z",
     "shell.execute_reply": "2020-08-27T01:37:22.453784Z"
    },
    "papermill": {
     "duration": 0.016895,
     "end_time": "2020-08-27T01:37:22.454403",
     "exception": false,
     "start_time": "2020-08-27T01:37:22.437508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt1 = 'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt1, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:37:22.476048Z",
     "iopub.status.busy": "2020-08-27T01:37:22.475179Z",
     "iopub.status.idle": "2020-08-27T01:38:38.836472Z",
     "shell.execute_reply": "2020-08-27T01:38:38.837163Z"
    },
    "papermill": {
     "duration": 76.375403,
     "end_time": "2020-08-27T01:38:38.837371",
     "exception": false,
     "start_time": "2020-08-27T01:37:22.461968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\"This is not only a scientific finding; it is also a very important finding because it will enable us to further study the phenomenon,\" said Dr. Jorge Llamas, from the National Institute of Anthropology and History (INAH) in Colombia, in a statement.\n",
      "\n",
      "\"We have previously found that humans have used human voices to communicate with the animals. In this case, the animals are communicating with us. In other words, this is a breakthrough in the field of animal communication,\" added Llamas...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85 \n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007411,
     "end_time": "2020-08-27T01:38:38.854046",
     "exception": false,
     "start_time": "2020-08-27T01:38:38.846635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## \"Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\"\n",
    "\n",
    "**Can we use GPT-2 to generate fake news stories?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:38:38.874787Z",
     "iopub.status.busy": "2020-08-27T01:38:38.873280Z",
     "iopub.status.idle": "2020-08-27T01:38:38.877494Z",
     "shell.execute_reply": "2020-08-27T01:38:38.876959Z"
    },
    "papermill": {
     "duration": 0.015914,
     "end_time": "2020-08-27T01:38:38.877596",
     "exception": false,
     "start_time": "2020-08-27T01:38:38.861682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt2 = 'Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt2, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:38:38.899726Z",
     "iopub.status.busy": "2020-08-27T01:38:38.899111Z",
     "iopub.status.idle": "2020-08-27T01:39:44.259753Z",
     "shell.execute_reply": "2020-08-27T01:39:44.260220Z"
    },
    "papermill": {
     "duration": 65.374842,
     "end_time": "2020-08-27T01:39:44.260368",
     "exception": false,
     "start_time": "2020-08-27T01:38:38.885526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today. The star was spotted trying on three dresses before attempting to walk out of the store.\n",
      "\n",
      "\n",
      "Abercrombie is one of a number of stores the star has frequented.\n",
      "\n",
      "\n",
      "The singer was spotted walking into Abercrombie & Fitch in West Hollywood just after noon this afternoon before leaving the store.\n",
      "\n",
      "\n",
      "The star is currently in the middle of a tour of Australia and New Zealand for her X Factor appearance on February 28....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85\n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007314,
     "end_time": "2020-08-27T01:39:44.275353",
     "exception": false,
     "start_time": "2020-08-27T01:39:44.268039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## \"Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\"\n",
    "\n",
    "**Can we use GPT-2 to imagine alternate histories of Lord of the Rings?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:39:44.293858Z",
     "iopub.status.busy": "2020-08-27T01:39:44.293034Z",
     "iopub.status.idle": "2020-08-27T01:39:44.297600Z",
     "shell.execute_reply": "2020-08-27T01:39:44.298022Z"
    },
    "papermill": {
     "duration": 0.015323,
     "end_time": "2020-08-27T01:39:44.298140",
     "exception": false,
     "start_time": "2020-08-27T01:39:44.282817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt3 = 'Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt3, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:39:44.316948Z",
     "iopub.status.busy": "2020-08-27T01:39:44.316085Z",
     "iopub.status.idle": "2020-08-27T01:41:17.350869Z",
     "shell.execute_reply": "2020-08-27T01:41:17.351674Z"
    },
    "papermill": {
     "duration": 93.046134,
     "end_time": "2020-08-27T01:41:17.351888",
     "exception": false,
     "start_time": "2020-08-27T01:39:44.305754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n",
      "\n",
      "Then the orcs made their move.\n",
      "\n",
      "The Great Orc Warband advanced at the sound of battle. They wore their weapons proudly on their chests, and they looked down upon their foes.\n",
      "\n",
      "In the distance, the orcs could be heard shouting their orders in a low voice.\n",
      "\n",
      "But the battle was not yet over. The orcs' axes and hammers slammed into the enemy ranks as though they were an army of ten thousand warriors, and their axes made the orcs bleed.\n",
      "\n",
      "In the midst of the carnage, the Elven leader Aragorn cried out: \"Come, brave. Let us fight the orcs!\"\n",
      "\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85 \n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007728,
     "end_time": "2020-08-27T01:41:17.368947",
     "exception": false,
     "start_time": "2020-08-27T01:41:17.361219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## \"For todayâ€™s homework assignment, please describe the reasons for the US Civil War.\"\n",
    "\n",
    "**Can we use GPT-2 to do our homework?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:41:17.389915Z",
     "iopub.status.busy": "2020-08-27T01:41:17.388795Z",
     "iopub.status.idle": "2020-08-27T01:41:17.392986Z",
     "shell.execute_reply": "2020-08-27T01:41:17.392511Z"
    },
    "papermill": {
     "duration": 0.015823,
     "end_time": "2020-08-27T01:41:17.393084",
     "exception": false,
     "start_time": "2020-08-27T01:41:17.377261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt4 = \"For todayâ€™s homework assignment, please describe the reasons for the US Civil War.\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt4, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T01:41:17.415594Z",
     "iopub.status.busy": "2020-08-27T01:41:17.415007Z",
     "iopub.status.idle": "2020-08-27T01:41:18.405282Z",
     "shell.execute_reply": "2020-08-27T01:41:18.404853Z"
    },
    "papermill": {
     "duration": 1.004293,
     "end_time": "2020-08-27T01:41:18.405405",
     "exception": false,
     "start_time": "2020-08-27T01:41:17.401112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: For todayâ€™s homework assignment, please describe the reasons for the US Civil War....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85 \n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007888,
     "end_time": "2020-08-27T01:41:18.421420",
     "exception": false,
     "start_time": "2020-08-27T01:41:18.413532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Now we can see why OpenAI is hesitate to release their full scale model (recall, over 1.5 billion parameters) to the general public...it has the potential to do a lot of harm when used with malevolent intentions. Luckily, they have still provided us smaller versions that allow us to demonstrate just how powerful Transformer models are when applied to natural language and how they provide an analytical space from which to study linguistics.**\n",
    "\n",
    "**Feel free to fork this notebook and experiment with your own inputs and model parameters to see what type of computer generated stories you can create!**"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007864,
     "end_time": "2020-08-27T01:41:18.437497",
     "exception": false,
     "start_time": "2020-08-27T01:41:18.429633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "\n",
    "**Below are the only references you need to implement your own GPT-2 model for text generation**\n",
    "\n",
    "\n",
    "> https://arxiv.org/abs/1904.09751  \n",
    "> https://openai.com/blog/better-language-models/  \n",
    "> https://huggingface.co/transformers/model_doc/gpt2.html  \n",
    "> https://huggingface.co/blog/how-to-generate  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 685.850705,
   "end_time": "2020-08-27T01:41:18.653488",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-08-27T01:29:52.802783",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0357f23486c1444c9924d8cc807b67f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "04b96f2bf3bd47ab9fa6948211b98145": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "07e566d1d8b241ea95d73284fd374af9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "131887c268164cd2a6283d07e5f3d287": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_553a0034b96f48d4b9ac1f9a9c28aa76",
        "IPY_MODEL_a41635e00bb243ebb1cd4a6d25036540"
       ],
       "layout": "IPY_MODEL_4958dab60f6a47c18e61ea4c465f4dc5"
      }
     },
     "1af0c790c94f46febd06c726829a698b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4e208435e48c4ff3b738355fa6e3e683",
        "IPY_MODEL_e8f7ccf5f45348d496f41f0ca39b3c53"
       ],
       "layout": "IPY_MODEL_b9c937e3efbf448ebe5d3641cedb2abe"
      }
     },
     "2269170e4ea2434e916473bdf60aab25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "25d6051ab72d42148606d86f46b98a94": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3aba15bea79d49d28fd25857bca39148": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d70dc3cd431943abbf3e1df57cd55a83",
        "IPY_MODEL_c62e9f679f3e4d7c8b49dea1f9d488a0"
       ],
       "layout": "IPY_MODEL_25d6051ab72d42148606d86f46b98a94"
      }
     },
     "48f3ef911cc04dc2ad2b9cac92b3b736": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4958dab60f6a47c18e61ea4c465f4dc5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4e208435e48c4ff3b738355fa6e3e683": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_48f3ef911cc04dc2ad2b9cac92b3b736",
       "max": 764.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_864d365585994a64ad312846db96df4c",
       "value": 764.0
      }
     },
     "553a0034b96f48d4b9ac1f9a9c28aa76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_abde418a2fd847ffac205a74e55a7b22",
       "max": 456318.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b33fdac701144794a613c002623c50a4",
       "value": 456318.0
      }
     },
     "5cd94184227a48678dfe1917bffd4080": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6dc38f21e1204eec9e839ba7b0dbafdb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "72d7fc5fb9ad4020ae1a6cda853dad87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_75c0c0d507cd40bc88844ed94c5278bb",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_04b96f2bf3bd47ab9fa6948211b98145",
       "value": " 3.10G/3.10G [01:27&lt;00:00, 35.6MB/s]"
      }
     },
     "75c0c0d507cd40bc88844ed94c5278bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b918064608a426fa3c7c1dab796a1a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "864d365585994a64ad312846db96df4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "8a84e8103df84141b2c70989b62d649d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a2c3585520ba4443a184b90311cae0a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "a41635e00bb243ebb1cd4a6d25036540": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7b918064608a426fa3c7c1dab796a1a7",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_2269170e4ea2434e916473bdf60aab25",
       "value": " 456k/456k [00:00&lt;00:00, 3.48MB/s]"
      }
     },
     "a639528193e94b1bacd27fb949a2c95b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "abde418a2fd847ffac205a74e55a7b22": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b33fdac701144794a613c002623c50a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "b96eb4e37a7a48d4b792a1805bda4a17": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0357f23486c1444c9924d8cc807b67f4",
       "max": 3096618024.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f43d4269b80e4b8098bd5d02d1ff85bb",
       "value": 3096618024.0
      }
     },
     "b9c937e3efbf448ebe5d3641cedb2abe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c62e9f679f3e4d7c8b49dea1f9d488a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8a84e8103df84141b2c70989b62d649d",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_07e566d1d8b241ea95d73284fd374af9",
       "value": " 1.04M/1.04M [00:00&lt;00:00, 2.10MB/s]"
      }
     },
     "ce13bba33eac46cfaf8df0fa1be8039f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d70dc3cd431943abbf3e1df57cd55a83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a639528193e94b1bacd27fb949a2c95b",
       "max": 1042301.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a2c3585520ba4443a184b90311cae0a0",
       "value": 1042301.0
      }
     },
     "e36b76cd1ea24effb43732cc57198f2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b96eb4e37a7a48d4b792a1805bda4a17",
        "IPY_MODEL_72d7fc5fb9ad4020ae1a6cda853dad87"
       ],
       "layout": "IPY_MODEL_6dc38f21e1204eec9e839ba7b0dbafdb"
      }
     },
     "e8f7ccf5f45348d496f41f0ca39b3c53": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5cd94184227a48678dfe1917bffd4080",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_ce13bba33eac46cfaf8df0fa1be8039f",
       "value": " 764/764 [01:28&lt;00:00, 8.65B/s]"
      }
     },
     "f43d4269b80e4b8098bd5d02d1ff85bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
